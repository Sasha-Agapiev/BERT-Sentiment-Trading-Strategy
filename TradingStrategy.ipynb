{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91BEbSZESg7N"
   },
   "source": [
    "## **Final Project: ETF Price Forecasting Using BERT-Derived Sentiment Analysis of Stock-Related Tweets**\n",
    "\n",
    "FRE-GY 7773: Machine Learning in Financial Engineering\n",
    "\n",
    "Professor Sandeep Jain\n",
    "\n",
    "Done by Sasha Agapiev (aba439) on 12/13/2022 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQPKX2O7SlMf"
   },
   "source": [
    "**ABSTRACT**\n",
    "\n",
    "With the saturation of sophisticated mathematical models and automatized trading algorithms, modern traders are turning to non-quantitative methods to gain an intellectual edge over the competition. A popular qualitative method has been sentiment analysis (or \"opinion mining\"), which is the practice of using natural language processing (NLP) to categorize text into either positive (1), neutral (0), or negative (-1). Traders have applied sentiment analysis techniques to Twitter threads, Reuters comment sections, and Federal Reserve reports to quickly analyze sentiment and to profit on news reports by trading based on the detected sentiment. Logically, news reports which yield a positive sentiment translate to long positions in the respective stock/ETF and vice-versa. In this analysis, I have applied modern sentiment analysis strategies to Federal Reserve statements in a back-testing environment to test the effictiveneness of such a method when it comes to time series forecasting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wlow-0-uaPyZ"
   },
   "source": [
    "### 1. **Importing the BERT Model to Enable Sentiment Prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9QmgA3Ko4NQ"
   },
   "source": [
    "### 1.1: Importing relevant packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23293,
     "status": "ok",
     "timestamp": 1670810793480,
     "user": {
      "displayName": "Sasha Agapiev",
      "userId": "18182803548370990209"
     },
     "user_tz": 300
    },
    "id": "HFU8owfgVnsy",
    "outputId": "2cc9e6e1-cdec-43bc-f503-d6d7ea57fcc3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\sasha\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "# The transformers package is required to run the BERT model\n",
    "\n",
    "!pip install transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11611,
     "status": "ok",
     "timestamp": 1670810819738,
     "user": {
      "displayName": "Sasha Agapiev",
      "userId": "18182803548370990209"
     },
     "user_tz": 300
    },
    "id": "8x7eYkupTyPC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorflow.keras.models import load_model\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKi7bxpwpKZH"
   },
   "source": [
    "### 1.2: Mounting Google Drive to Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9266,
     "status": "ok",
     "timestamp": 1670810831824,
     "user": {
      "displayName": "Sasha Agapiev",
      "userId": "18182803548370990209"
     },
     "user_tz": 300
    },
    "id": "bcBxhKGIpAWm",
    "outputId": "1dae722a-8e21-40ce-f3f9-f178f258f594"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_AIjtzjpPuO"
   },
   "source": [
    "### 1.3: Loading the trained BERT model by invoking TensorFlow's load_model()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234,
     "referenced_widgets": [
      "30d5d92f4e96402685d4ceaed3031d9c",
      "cf26c77fcc9b4a549d25884d4926a847",
      "c26527cceef94f27bd3b8156e8b10ab9",
      "50d5f844b6724cf094116d42804e7ae5",
      "b1880a6e3e844656a67268bff6a996ae",
      "fff2baa8506b4908a67755c6396f766f",
      "0928b00e72d54a7cae7193cc7a4fa22d",
      "9e760aa850b649cdabae98d5d56c535a",
      "6eb7a4645e70445a969314e0a43f288c",
      "b0012d6bcd7043d19b2d3bdb0847d0cf",
      "a86e9b5cf74b431cb4593b570d3626ed",
      "1f4cceeba4b147a1841541aa60f4d75a",
      "8f5ce0336f28417aafbf0b8e58701c8e",
      "8fbc2898001c4e12bd0e0c37eac74b38",
      "3b9d4bc8ddb646dab982382c9eb81a87",
      "97fcb66025064044aa7a85c1ac46ae0b",
      "30c7a29953144ab4aad4af8d57ea2c62",
      "26cc53bea7954a30acd936cd8e07b181",
      "8a8a9936fecc4bb99ab2f3d7e03be7b4",
      "d814e08940794e478d0f3ed720d904d0",
      "a522e2a0fdc54c9892a54abd44522fa8",
      "fb92c4f3023643c29ba3aa5f77c9dae0",
      "30ad9e5938d745bf9994545b77de8369",
      "94f99d6f763a49d9879c04bf63707116",
      "eeeb7851ead6451fa96d009685a0c93a",
      "18bc653c1de4498dab7f9277bb9191b1",
      "37678a4dba3948009342ec257216194d",
      "8d520355c15b41eea0db2c8598c97dc0",
      "478cbbb32cc5454fa97cc696a4f1a09c",
      "afbac4652c0f46919c2367c281841314",
      "48dd2621c76a41f59ab7aff452fbfbcd",
      "4179dc58db0d4d218d36dbd4680b18a2",
      "80db2dfb246144fbbff9ffc705e06cff",
      "d7faac7f420d494c9f8e22b06009eb2f",
      "c5c83b5dc2a34a9f91ca5d6d2a48e997",
      "53fbfba48ac84200993e21b472c0fdc9",
      "97548961b41a4dcdb264a9695b8c1247",
      "84855720050440f4af465f79a578faaa",
      "e2cfe5eec4544d88817b5e468291dae7",
      "e235da18c828488bbe43837385b4747e",
      "4508957f03954dcfb12a0de9f9e636f0",
      "5fca222f2e8f4174b32e1bbd9a576812",
      "faf97e109a6a419dadfe1dccff37a3d4",
      "ad2e4bc1656c4c538e6a823da4b4dfad"
     ]
    },
    "executionInfo": {
     "elapsed": 22410,
     "status": "ok",
     "timestamp": 1670810857009,
     "user": {
      "displayName": "Sasha Agapiev",
      "userId": "18182803548370990209"
     },
     "user_tz": 300
    },
    "id": "9HCetHmZVfoT",
    "outputId": "292bbfac-626f-4c3c-bab9-289a4949eb50"
   },
   "outputs": [],
   "source": [
    "# Loading-in the Trained BERT model by calling load_weights\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model.load_weights('/content/drive/MyDrive/YEAR 4/ML/Assignments/FINAL PROJECT/BERT_Model_weights.h5') \n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qz6F4Febpo7u"
   },
   "source": [
    "*1.4: Defining helper functions to generate predictions using the BERT model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 378,
     "status": "ok",
     "timestamp": 1670810862732,
     "user": {
      "displayName": "Sasha Agapiev",
      "userId": "18182803548370990209"
     },
     "user_tz": 300
    },
    "id": "r9rThSD4ZQaz"
   },
   "outputs": [],
   "source": [
    "# ~\n",
    "# print_sentiment_predictions() is a helper function which takes two arguments:\n",
    "#         sentences_to_predict: A list of strings (tweets) for which to calculate sentiment predictions\n",
    "#         to_print: A boolean which we set to True if we want to print the prediction labels for each sentence\n",
    "# If to_print is set to false, the function will return a list of 0s and 1s, \n",
    "# representing the sentiment predictions for each sentence in sentences_to_predict.\n",
    "# The function works by applying a softmax layer to the BERT model. This softmax\n",
    "# layer predicts an argmax of 0 if the sentiment for a given sentence is \n",
    "# determined to be negative, and 1 if it is determined to be positive.  \n",
    "# ~\n",
    "def get_sentiment_predictions(sentences_to_predict, to_print=False):\n",
    "  max_len = 128       # The max acceptable length of a sentence in sentences_to_predict. Any input greater than this will cause the tokenizer to fail\n",
    "  tf_batch = tokenizer(sentences_to_predict, max_length=max_len, padding=True, truncation=True, return_tensors='tf') # Tokenizing sentences_to_predict\n",
    "  tf_outputs = bert_model(tf_batch) # Bert model's outputs, calculated using the tokenized input sentences\n",
    "  tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1) # Applying the softmax layer to the Bert model's outputs\n",
    "  labels = ['Negative','Positive'] # Labels for the softmax layer (0 is negative sentiment, 1 is positive)\n",
    "  label = tf.argmax(tf_predictions, axis=1)  # Generating a list of labels using the softmax output\n",
    "  label = label.numpy()                      # Converting the list of labels to a numpy array\n",
    "  if to_print:                               # Check if we want to print the predictions, or return the list of labels\n",
    "    for i in range(len(sentences_to_predict)):\n",
    "      print(sentences_to_predict[i], \": \\n\", labels[label[i]])\n",
    "  else:\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLCv7go-a9bq"
   },
   "source": [
    "*1.5: Testing if the sentiment prediction function works by running it on a list of fake tweets with varying sentiments.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 374,
     "status": "ok",
     "timestamp": 1670810868736,
     "user": {
      "displayName": "Sasha Agapiev",
      "userId": "18182803548370990209"
     },
     "user_tz": 300
    },
    "id": "YeKmxBTCZGC8"
   },
   "outputs": [],
   "source": [
    "# test_pred_sentences are fake tweets that I made up to get a sense of how well\n",
    "# the BERT model predicts sentiment. The first and third sentences both clearly\n",
    "# exude positive sentiment, while the second sentence exudes negative sentiment.\n",
    "test_pred_sentences = ['Super excited about this awesome stock, I know for a fact it will go up this week!',\n",
    "                  'Horrible, terrible investment choice, would be better off buying an NFT', \n",
    "                  'Oh my god, Gamestop is doing so well right now I cant wait to invest more',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 427,
     "status": "ok",
     "timestamp": 1670810871395,
     "user": {
      "displayName": "Sasha Agapiev",
      "userId": "18182803548370990209"
     },
     "user_tz": 300
    },
    "id": "OWtljGhqZySt",
    "outputId": "ac1005c3-79b7-44ab-ca27-e70c3c724492"
   },
   "outputs": [],
   "source": [
    "# Call get_sentiment_predictions with print mode turned on to see if the model\n",
    "# works as expected\n",
    "get_sentiment_predictions(test_pred_sentences, to_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NcWl4p2DDtJA"
   },
   "source": [
    "Thankfully, the BERT model labels these test sentences as we would expect it to. We can also see how these predictions correspond to the list of labels generated by the softmax layer by calling get_sentiment_predictions\n",
    "with print mode turned off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1089,
     "status": "ok",
     "timestamp": 1670810875536,
     "user": {
      "displayName": "Sasha Agapiev",
      "userId": "18182803548370990209"
     },
     "user_tz": 300
    },
    "id": "lAcMf1IqD_VA",
    "outputId": "bd866e52-a5b7-4c82-d67a-ea205b5879e2"
   },
   "outputs": [],
   "source": [
    "get_sentiment_predictions(test_pred_sentences, to_print=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWLQrDDUeIEr"
   },
   "source": [
    "~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ \n",
    "**PART 2: Importing Dataset of 7 million Stock-Related Tweets (tweeted from May 2018 to September 2018)**\n",
    "\n",
    "~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdY8Y5TSIGhq"
   },
   "source": [
    "*2.1: Defining a helper function to import tweets from the data set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 324,
     "status": "ok",
     "timestamp": 1670810881055,
     "user": {
      "displayName": "Sasha Agapiev",
      "userId": "18182803548370990209"
     },
     "user_tz": 300
    },
    "id": "p20yvAIiEFnt"
   },
   "outputs": [],
   "source": [
    "# ~\n",
    "# import_tweet_dataset() is a helper function which takes one argument:\n",
    "#         already_created: Has this dataframe already been created\n",
    "# Creating the dataframe directly from the tweet data set csv file takes\n",
    "# a very long time, and to avoid having to do this every time we run the \n",
    "# notebook we can choose to read import tweets_df from Google Drive by \n",
    "# setting already_created to True. If we set already_created to False, \n",
    "# then we are starting this process over from scratch (which takes much \n",
    "# longer).\n",
    "# ~\n",
    "def import_tweet_dataset(already_created=True):\n",
    "  if already_created:\n",
    "    tweets_df = pd.read_csv('/content/drive/MyDrive/YEAR 4/ML/Assignments/FINAL PROJECT/tweets_df.csv')\n",
    "    tweets_df.set_index(\"Date\", inplace=True)\n",
    "    return tweets_df \n",
    "  else:\n",
    "    # Using Pandas read_csv() to create a Dataframe of tweets\n",
    "    tweets_df = pd.read_csv('/content/drive/MyDrive/YEAR 4/ML/Assignments/FINAL PROJECT/tweets.csv', on_bad_lines=\"skip\")\n",
    "    # Dropping irrelevant columns\n",
    "    tweets_df.drop(['id', 'user_id', 'in_reply_to_status_id', 'in_reply_to_user_id', 'retweeted_status_id', 'retweeted_user_id', 'source'], axis=1, inplace=True)\n",
    "    # Dropping tweets that are not labelled as English because our BERT model is not trained to understand these\n",
    "    tweets_df = tweets_df.loc[tweets_df['lang'] == 'en'] \n",
    "    tweets_df.drop(['lang'], axis=1, inplace=True)\n",
    "    # Parsing \"created_at dates into Datetime format and set them as the DataFrame's index to help with SPY forecasting\n",
    "    tweets_df['Date'] = pd.to_datetime(tweets_df['created_at']).dt.strftime('%Y-%m-%d') # Only interested in the year, month, and day of tweets' creation dates\n",
    "    # Setting tweet dates to act as the index for the Dataframe for when we apply the trading strategy\n",
    "    tweets_df.set_index(\"Date\", inplace=True)\n",
    "    # Dropping the full creation time column since we no longer need this\n",
    "    tweets_df.drop(['created_at'], axis=1, inplace=True)\n",
    "    # Dropping duplicate and NaN values\n",
    "    tweets_df = tweets_df.drop_duplicates()\n",
    "    tweets_df = tweets_df.dropna()\n",
    "    # Saving tweets_df to disk/Drive so we can call import_tweet_dataset(already_created=True) next time we run the notebook\n",
    "    # tweets_df will be saved to disk as 'tweets_df.csv' \n",
    "    tweets_df.to_csv('tweets_df.csv', encoding='utf-8')\n",
    "    return tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxs-dq77Ihia"
   },
   "source": [
    "*2.2: Calling the helper function based on whether we want to create the Dataframe from scratch, or import it from Disk/Drive.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16598,
     "status": "ok",
     "timestamp": 1670810919877,
     "user": {
      "displayName": "Sasha Agapiev",
      "userId": "18182803548370990209"
     },
     "user_tz": 300
    },
    "id": "qw1cn7sRH4DP"
   },
   "outputs": [],
   "source": [
    "# tweets_df = import_tweet_dataset(already_created=False)\n",
    "tweets_df = import_tweet_dataset(already_created=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkwhS5v8pzpA"
   },
   "source": [
    "*2.3: Getting a sense of the tweets in the Dataframe by calling df['text'].describe().*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9169,
     "status": "ok",
     "timestamp": 1670810932741,
     "user": {
      "displayName": "Sasha Agapiev",
      "userId": "18182803548370990209"
     },
     "user_tz": 300
    },
    "id": "FQP9Fr7PqWJg",
    "outputId": "9ac85e59-458c-4bdc-8b7b-c0b1c247496a"
   },
   "outputs": [],
   "source": [
    "tweets_df['text'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6iCQVoHqw3S"
   },
   "source": [
    "*2.4: Displaying the first and last 10 rows for unsupervised analysis.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1670810934936,
     "user": {
      "displayName": "Sasha Agapiev",
      "userId": "18182803548370990209"
     },
     "user_tz": 300
    },
    "id": "fGr9wNzwqsT-",
    "outputId": "2321409b-5d79-4e0e-cd36-2143a46420c9"
   },
   "outputs": [],
   "source": [
    "tweets_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1670810938570,
     "user": {
      "displayName": "Sasha Agapiev",
      "userId": "18182803548370990209"
     },
     "user_tz": 300
    },
    "id": "jv1BJ5rlqufi",
    "outputId": "42904f93-8abc-4627-b9be-0b527918ae34"
   },
   "outputs": [],
   "source": [
    "tweets_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSgwHI_Zs8Hb"
   },
   "source": [
    "*2.5: Finally, we should test the sentiment predict functionality on some of the tweets.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 258,
     "status": "ok",
     "timestamp": 1670810941107,
     "user": {
      "displayName": "Sasha Agapiev",
      "userId": "18182803548370990209"
     },
     "user_tz": 300
    },
    "id": "4W7eRlCyu4Um"
   },
   "outputs": [],
   "source": [
    "#~\n",
    "# get_tweets_by_date() is a helper function which takes a datetime input\n",
    "# and returns a numpy array of all tweets in the tweet dataframe which were \n",
    "# posted on this date.\n",
    "#~\n",
    "import re\n",
    "def get_tweets_by_date(datetime):\n",
    "  tweets_unparsed = tweets_df.loc[datetime].to_numpy() # Get a numpy array of all the tweets from this date\n",
    "  tweets_parsed = []\n",
    "  for t in tweets_unparsed:\n",
    "    unparsed_text = t[0]\n",
    "    parsed_text1 = re.sub(r'http\\S+', '', unparsed_text) # Removing URLs from tweets\n",
    "    parsed_text = re.sub('@[^\\s]+','', parsed_text1) # Removing usernames and mentions from tweets\n",
    "    parsed_trimmed_text = (parsed_text[:128] + '') if len(parsed_text) > 128 else parsed_text # Only keep tweets with length <= 128 characters to avoid spam\n",
    "    tweets_parsed.append(parsed_trimmed_text)\n",
    "  return tweets_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5401,
     "status": "ok",
     "timestamp": 1670810949239,
     "user": {
      "displayName": "Sasha Agapiev",
      "userId": "18182803548370990209"
     },
     "user_tz": 300
    },
    "id": "7Br9S_zbzk5d",
    "outputId": "35e9573b-782f-45dc-a565-7a946759f424"
   },
   "outputs": [],
   "source": [
    "# Test the sentiment prediction on the first 10 tweets from September 12, 2017\n",
    "test_arr = get_tweets_by_date('2017-09-12')\n",
    "test_arr = test_arr[:10]\n",
    "get_sentiment_predictions(test_arr, to_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2409,
     "status": "ok",
     "timestamp": 1670810954572,
     "user": {
      "displayName": "Sasha Agapiev",
      "userId": "18182803548370990209"
     },
     "user_tz": 300
    },
    "id": "nYx00H_88oJf",
    "outputId": "ae95c244-0258-4741-dda0-480701012b47"
   },
   "outputs": [],
   "source": [
    "get_sentiment_predictions(test_arr, to_print=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeTegw1N8uId"
   },
   "source": [
    "As we can see from the test code above, our BERT model can succesfully predict the sentiment of a given array of tweets! Although it is impossible to quantitatively prove whether these sentiment predictions are correct since the dataset is not labelled, we can logically see that most of the predictions make sense. \n",
    "\n",
    "For instance, **\"NY Times says #Amazon should build in CO\"** is most likely a positive tweet since the NYT is implying that Amazon will expand to a new state which should lead to an increase in sales, and **\"Feds To Partially Blame Tesla's Autopilot In Fatal Crash: Report $TSLA\"** would probably qualify as negative news for Tesla for obvious reasons.\n",
    "\n",
    "Nevertheless, there will always be some results that seem off, but this is to be expected since our BERT model only had 83.5% validation accuracy on StockTwits data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPhCvSbfoIUB"
   },
   "source": [
    "~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~  \n",
    "\n",
    "**PART 3: Applying a Sentiment-Based ETF Forecasting Strategy for SPY and DJIA**\n",
    "\n",
    "~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bK1gtEi3oHPF"
   },
   "source": [
    "*3.1: We first need to import SPY and DJIA price data into a single Pandas dataframe.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 262,
     "status": "ok",
     "timestamp": 1670810958256,
     "user": {
      "displayName": "Sasha Agapiev",
      "userId": "18182803548370990209"
     },
     "user_tz": 300
    },
    "id": "RpS3joqmoGkF"
   },
   "outputs": [],
   "source": [
    "# ~\n",
    "# read_data() is a helper function that takes a ticker name as its only argument, \n",
    "# then reads in this ticker's data from Google Drive and returns a pandas dataframe\n",
    "# ~\n",
    "def read_data(ticker):\n",
    "  path = '/content/drive/MyDrive/YEAR 4/ML/Assignments/FINAL PROJECT/' + ticker + '.csv'\n",
    "  data = pd.read_csv(path, parse_dates=['Date'], index_col='Date')\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 476,
     "status": "ok",
     "timestamp": 1670810966270,
     "user": {
      "displayName": "Sasha Agapiev",
      "userId": "18182803548370990209"
     },
     "user_tz": 300
    },
    "id": "MKdoroWMLSgG"
   },
   "outputs": [],
   "source": [
    "# ~\n",
    "# import_ETF_data() is a helper function that calls read_data to \n",
    "# retrieve SPY and DJIA price data from a CSV file and loads it \n",
    "# into a Pandas DataFrame. Then, the function uses this price\n",
    "# DataFrame to create a daily returns DataFrame for SPY and DJIA\n",
    "# daily prices. The function returns the two DataFrames, ETF_prices\n",
    "# and ETF_returns.\n",
    "# ~\n",
    "\n",
    "def import_ETF_data():\n",
    "  SPY_data = read_data(\"SPY_price_data\")\n",
    "  DJIA_data = read_data(\"DJIA_price_data\")\n",
    "  ETF_prices = SPY_data.drop(['PX_VOLUME'], axis=1)\n",
    "  ETF_prices[\"DJIA Prices\"] = DJIA_data['PX_LAST']\n",
    "  ETF_prices.rename(columns={'PX_LAST':'SPY Prices'}, inplace=True)\n",
    "  ETF_prices = ETF_prices.loc[\"2017-05-14\":\"2017-09-17\", :]\n",
    "  ETF_returns = pd.DataFrame(data=np.zeros(shape=(len(ETF_prices.index), ETF_prices.shape[1])), \n",
    "                              columns=ETF_prices.columns.values,\n",
    "                              index=ETF_prices.index)\n",
    "  for col in ETF_returns.columns:\n",
    "    ETF_returns[col] = ETF_prices[col].pct_change()\n",
    "  ETF_returns = ETF_returns.dropna()\n",
    "  ETF_returns.rename(columns={\"SPY Prices\": \"SPY Returns\", \"DJIA Prices\" : \"DJIA Returns\"}, inplace=True)\n",
    "  return ETF_prices, ETF_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1867,
     "status": "ok",
     "timestamp": 1670810970524,
     "user": {
      "displayName": "Sasha Agapiev",
      "userId": "18182803548370990209"
     },
     "user_tz": 300
    },
    "id": "_7iely8LLt9n"
   },
   "outputs": [],
   "source": [
    "ETF_prices, ETF_returns = import_ETF_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxJJ9xy7MIpC"
   },
   "source": [
    "*3.2: Testing if the data imported correctly by displaying the first and last 5 rows of ETF_prices and ETF_returns.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "executionInfo": {
     "elapsed": 357,
     "status": "ok",
     "timestamp": 1670810974790,
     "user": {
      "displayName": "Sasha Agapiev",
      "userId": "18182803548370990209"
     },
     "user_tz": 300
    },
    "id": "D-dV4oxc_aa0",
    "outputId": "0d50c6b8-a223-4c38-b1e9-9e678a8ada6b"
   },
   "outputs": [],
   "source": [
    "ETF_prices.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1670810978241,
     "user": {
      "displayName": "Sasha Agapiev",
      "userId": "18182803548370990209"
     },
     "user_tz": 300
    },
    "id": "nwMABRLd_dId",
    "outputId": "e3487bbb-96a9-4d37-b67c-db513ee3946e"
   },
   "outputs": [],
   "source": [
    "ETF_prices.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "executionInfo": {
     "elapsed": 378,
     "status": "ok",
     "timestamp": 1670810980962,
     "user": {
      "displayName": "Sasha Agapiev",
      "userId": "18182803548370990209"
     },
     "user_tz": 300
    },
    "id": "dZrUv7AcWWSc",
    "outputId": "a9138859-da93-4d4f-add4-ec461403329c"
   },
   "outputs": [],
   "source": [
    "ETF_returns.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "executionInfo": {
     "elapsed": 257,
     "status": "ok",
     "timestamp": 1670810983849,
     "user": {
      "displayName": "Sasha Agapiev",
      "userId": "18182803548370990209"
     },
     "user_tz": 300
    },
    "id": "pDtgUMxXMXov",
    "outputId": "68664af2-c6a2-4e00-ab12-1b7eb3301f71"
   },
   "outputs": [],
   "source": [
    "ETF_returns.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKhvibH4BqIH"
   },
   "source": [
    "*3.4: With our data correctly stored in a Dataframe, we can define the helper function which will power our trading strategy.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1670810988773,
     "user": {
      "displayName": "Sasha Agapiev",
      "userId": "18182803548370990209"
     },
     "user_tz": 300
    },
    "id": "_54msk1aBpk0"
   },
   "outputs": [],
   "source": [
    "# ~\n",
    "# are_tweets_positive() is a helper function which calculates whether the \n",
    "# tweets on a given day more positive than negative. It takes the following\n",
    "# arguments:\n",
    "#         datetime: The date we want to get a sentiment signal for \n",
    "#         threshold_ratio: The ratio of positive to negative tweets which needs to be exceeded in order to get a positive sentiment signal\n",
    "#         cutoff_ratio: The percentage of tweets we want to analyze for the day (1 = all the tweets tweeted on the given day, 0.5 = half, etc...)\n",
    "#         debugging: A boolean which we set to True if we want to see helpful print statements\n",
    "#         return_ratio: A boolean which we set to True if we want the function to return the positive to negative ratio itself, rather than a signal\n",
    "# The function will return a trading signal depending on these arguments.\n",
    "# For example, if we call are_tweets_positive('2017-09-12', 0.8, 0.5), the \n",
    "# function will return True if more than 80% of tweets on 2017-09-12 are positive \n",
    "# and false otherwise, using only 50% of tweets for that day to get the calculation.\n",
    "# If we want to get the positive to negative ratio as a number, set return_ratio\n",
    "# equal to True.\n",
    "# The reason why we use cutoff_ratio is because there are ~70,000 stock-related \n",
    "# tweets posted every day, so we can choose to omit some of them to speed-up \n",
    "# the computation.\n",
    "# For increased speed, turn on Colab GPU optimization in Notebook settings. \n",
    "# ~\n",
    "def are_tweets_positive(datetime, threshold_ratio=0.5, cutoff_ratio=0.1, debugging=False, return_ratio=False):\n",
    "  tweets_pre = get_tweets_by_date(datetime)   # Get the data set of tweets for the given datetime\n",
    "  tweets = np.asarray(tweets_pre)             # Convert the data set into numpy array so we can use np.array_split\n",
    "  chunk_size = len(tweets) / 100              # Split the data set of daily tweets into \"chunks\" of 100 tweets\n",
    "  chunks = np.array_split(tweets, chunk_size) # This is because we can only predict lists of 100 at a time or else Colab runs out of RAM\n",
    "  cutoff_amount = int(cutoff_ratio * len(chunks)) # If cutoff_ratio is 1 (100%), then we use the whole dataset for the day.\n",
    "  positive_count = 0                          # Total number of tweets labelled as positive\n",
    "  total_count = 0                             # Total number of tweets seen\n",
    "  num_iters = 0                               # Number of chunks predicted so far (used for debugging)\n",
    "  for chunk in chunks[:cutoff_amount]:        # Iterate through each chunk (if cutoff_ratio=1, then we predict every chunk for the day)\n",
    "    if debugging:                             # Only print status if debugging is set to True\n",
    "      if num_iters % 5 == 0:\n",
    "        print(\"CHUNKS PROCESSED: \", num_iters)\n",
    "    num_iters += 1\n",
    "    flat_chunk = chunk.tolist()               # Turn the chunk into a list so we can apply softmax to it and get a prediction\n",
    "    chunk_predictions = get_sentiment_predictions(flat_chunk) \n",
    "    for p in chunk_predictions:               # chunk_predictions is just a list of 1s (positive sentiment tweets) and 0s (negative sentiment tweets)\n",
    "      if p == 1:          \n",
    "        positive_count += 1\n",
    "      total_count += 1\n",
    "  positive_ratio = positive_count / total_count # The \"positive tweet ratio\" is just the proportion of tweets in a day that are positive\n",
    "  if debugging:\n",
    "    print(\"POSITIVE RATIO: \", positive_ratio)\n",
    "  if return_ratio:                              # If return_ratio is set to True, then we return positive_ratio as a number\n",
    "    return positive_ratio \n",
    "  if positive_ratio > threshold_ratio:          # Else, we return True is positive_ratio > threshold_ratio and False otherwise\n",
    "    return True\n",
    "  else:\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-2CUhmb7FGN"
   },
   "source": [
    "*3.5: Testing the functionality of are_tweets_positive on a highly-reduced daily tweet dataset and with debugging functionality turned on to get a sense of performance.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1687834,
     "status": "ok",
     "timestamp": 1670812684559,
     "user": {
      "displayName": "Sasha Agapiev",
      "userId": "18182803548370990209"
     },
     "user_tz": 300
    },
    "id": "BLpY6JRFDlqN",
    "outputId": "6f25261a-22df-4bcf-b7ec-9528c4790e8d"
   },
   "outputs": [],
   "source": [
    "# ~\n",
    "# Using a cutoff_ratio of 0.1 means we are getting the positive tweet ratio using only 10% of the dataset\n",
    "# for the given day (2017-09-12). Using a threshold ratio of 0.5 means we get a positive signal if \n",
    "# the positive ratio for the day is greater than 50%. Setting debugging to True just means we are\n",
    "# printing the outputs to get a sense of performance\n",
    "# ~\n",
    "pos = are_tweets_positive('2017-09-12', threshold_ratio=0.5, cutoff_ratio=0.1, debugging=True)\n",
    "if pos:\n",
    "  print(\"The tweets on 2017-09-12 give a positive trading signal!\")\n",
    "else:\n",
    "  print(\"The tweets on 2017-09-12 give a negative trading signal :(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPGhgCHhIqCJ"
   },
   "source": [
    "*3.6: Graphically visualizing how different cutoff ratios affect the difference between the true positive ratio for a day (i.e: the positive ratio we get when using a cutoff ratio of 1, meaning we use the entire tweet data set of ~70,000 tweets for the day) and the positive ratio on the reduced data set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1670812714561,
     "user": {
      "displayName": "Sasha Agapiev",
      "userId": "18182803548370990209"
     },
     "user_tz": 300
    },
    "id": "GMIxceyverND"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "cutoffs = [0.05, 0.1, 0.25, 0.5, 0.75, 0.8, 0.9]\n",
    "cutoffs_to_plot = [100*c for c in cutoffs] # We want to plot the cutoffs as percentages\n",
    "\n",
    "# ~\n",
    "# test_cutoff_performance() is a helper function that takes two arguments:\n",
    "#       cutoffs: A list of cutoffs (0 < cutoff <= 1) representing the cutoff ratios that we would like to test \n",
    "#       datetime: The day we want to perform the test on\n",
    "# The function has no return values, but instead it just plots a graph of positive ratios vs cutoffs for the day, \n",
    "# and compares this with the true positive ratio. We use this function to graphically visualize how different \n",
    "# cutoff ratios affect the difference between the true positive ratio and the positive ratio given by data sets\n",
    "# of variangly-reduced sizes.\n",
    "# ~\n",
    "def test_cutoff_performance(cutoffs, datetime):\n",
    "  positive_ratios = []\n",
    "  true_ratio = are_tweets_positive(datetime, cutoff_ratio=1, return_ratio=True)  # The positive_ratio we get when we use all tweets for the day (full dataset)\n",
    "  true_ratio_list = [true_ratio] * len(cutoffs) # As a constant list (for plotting)\n",
    "  for c in cutoffs:\n",
    "    pos_ratio = are_tweets_positive(datetime, cutoff_ratio=c, return_ratio=True) # Calculate the positive ratio for each cutoff ratio in cutoffs\n",
    "    positive_ratios.append(pos_ratio) # And we add this to the list of positive ratios\n",
    "  plt.rcParams[\"figure.figsize\"] = [7.00, 3.50] # Create a plot to display the results\n",
    "  plt.rcParams[\"figure.autolayout\"] = True\n",
    "  plt.plot(cutoffs_to_plot, positive_ratios, c='g', label=\"Positive Ratio\")\n",
    "  plt.plot(cutoffs_to_plot, true_ratio_list, c='purple', label=\"Positive Ratio When Using Full Data Set\")\n",
    "  titlestr = \"Positive Ratio of Tweets on \" + datetime + \" When Using Varying Dataset Cutoffs\" \n",
    "  plt.title(titlestr)\n",
    "  plt.xlabel(\"Cutoff (Percent of Daily Tweet Data Set Used)\")\n",
    "  plt.ylabel(\"Positive Ratio of Tweets\")\n",
    "  plt.legend(loc=\"best\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdy47dkrMinA"
   },
   "outputs": [],
   "source": [
    "# We want to test how the cutoff ratios play a role on five different days\n",
    "\n",
    "#test_cutoff_performance(cutoffs, '2017-09-12')\n",
    "#test_cutoff_performance(cutoffs, '2017-09-11')\n",
    "#test_cutoff_performance(cutoffs, '2017-09-10')\n",
    "#test_cutoff_performance(cutoffs, '2017-09-09')\n",
    "#test_cutoff_performance(cutoffs, '2017-09-08')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTQxLjyzDrwA"
   },
   "source": [
    "*3.7: We see that cutoff ratio affects the accuracy of the positive ratio, but to a very minimal extent. This means there is no point in using the whole tweet dataset for each day to get our trading signals since this takes a huge amount of time computationally with little added benefit. Thus, we will use a cutoff ratio of 0.5 when calculating the trading signals in the next step, and we will store these trading signals to disk so we don't have to compute them more than once.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 278,
     "status": "ok",
     "timestamp": 1670812724110,
     "user": {
      "displayName": "Sasha Agapiev",
      "userId": "18182803548370990209"
     },
     "user_tz": 300
    },
    "id": "yzccJmObEtHw"
   },
   "outputs": [],
   "source": [
    "# ~\n",
    "# get_sentiment_trading_signals() is a helper function that takes two arguments:\n",
    "#         cutoff: The data cutoff ratio (percentage) to be used when calculating the positive ratio of tweets for a day\n",
    "#         lookback_amount: How far to lookback when getting a sentiment calculation for a trading day\n",
    "# The function will return a list of positive_ratios for all the days in the\n",
    "# ETF_prices DataFrame, using the lookback_amount. \n",
    "# For instance, using a lookback_amount of 1 would mean that for a given trading \n",
    "# day d, we calculate the sentiment trading signal for this day by calling \n",
    "# are_tweets_positive on the tweet data set for d-1 (i.e: 1 day prior). If \n",
    "# lookback_amount is set to 7, then we get the sentiment trading signal for \n",
    "# the day by analyzing the sentiment of tweets from one week prior. \n",
    "# In our trading strategy, we use a lookback_amount of 1 because tweets are \n",
    "# very reactive and so they reflect current trends well, and also because \n",
    "# a lookback_amount of 1 day is common in papers that discuss Twitter sentiment\n",
    "# analysis for trading strategies. \n",
    "# ~\n",
    "from datetime import datetime, timedelta\n",
    "def get_sentiment_trading_signals(cutoff, lookback_amount):\n",
    "  signals = []\n",
    "  for date in ETF_prices.index[5:-5].strftime('%Y-%m-%d'):\n",
    "    previous_day = datetime.strptime(date, '%Y-%m-%d') - timedelta(days=lookback_amount) # Get the previous day\n",
    "    previous_day = previous_day.strftime('%Y-%m-%d') # Convert previous_day to datetime string\n",
    "    signal = are_tweets_positive(previous_day, cutoff_ratio=cutoff, return_ratio=True)\n",
    "    signals.append(signal)\n",
    "  return signals\n",
    "\n",
    "# ~\n",
    "# create_signals_list() is a helper function that calls get_sentiment_trading_signals\n",
    "# to get a list of sentiment signals if already_created==False, and then saves\n",
    "# this list to disk. If already_created == True, then the function simply\n",
    "# creates the list by reading input from the file on disk. This is to save\n",
    "# time and computational resources since creating the signals list \n",
    "# takes over 1 hour. \n",
    "# ~\n",
    "def create_signals_list(cutoff=0.5, lookback_amount=1, already_created=True):\n",
    "  if already_created:\n",
    "    signals = []\n",
    "    f = open('sentiment_list.txt', 'r')\n",
    "    print(f.read())\n",
    "    f.close()\n",
    "    return(signals)\n",
    "  else:\n",
    "    signals = get_sentiment_trading_signals(cutoff, lookback_amount)\n",
    "    with open('sentiment_list.txt', 'w+') as f:\n",
    "      for signal in signals:\n",
    "        f.write('%d' %signal)\n",
    "    print(\"File written successfully\")\n",
    "    f.close()\n",
    "    return(signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZjspnhRExfw"
   },
   "outputs": [],
   "source": [
    "signals = get_sentiment_trading_signals(0.5, 1)\n",
    "print(signals)\n",
    "# create_signals_list(already_created=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajoANdqCQhh3"
   },
   "source": [
    "*3.8: With the sentiment trading signals calculated, let us do some unsupervised analysis to see how these signals correlate with actual ETF prices and returns.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h6Lb9SvY9jdd"
   },
   "outputs": [],
   "source": [
    "# ~\n",
    "# pos_ratio_vs_day() is a helper function that takes a list of sentiment\n",
    "# signals and plots them against ETF prices and returns. This is helpful\n",
    "# for unsupervised analysis of how tweet sentiment correlates to ETF \n",
    "# behavior\n",
    "# ~\n",
    "def pos_ratio_vs_day(signals): \n",
    "  dates = []\n",
    "  positive_ratios_100 = []\n",
    "  positive_ratios = []\n",
    "  for date in ETF_prices.index[5:75].strftime('%Y-%m-%d'):\n",
    "    dates.append(date)\n",
    "    positive_ratio = signals[0]\n",
    "    positive_ratios_100.append((positive_ratio*100))\n",
    "    positive_ratios.append(positive_ratio)\n",
    "  SPY_prices = ETF_prices[5:75][\"SPY Prices\"].values\n",
    "  SPY_returns = ETF_returns[5:75][\"SPY Returns\"].values\n",
    "  DJIA_prices = ETF_prices[5:75][\"DJIA Prices\"].values\n",
    "  DJIA_returns = ETF_returns[5:75][\"DJIA Returns\"].values\n",
    "  DJIA_returns = DJIA_returns*100 \n",
    "  SPY_returns = SPY_returns*100\n",
    "  fig, ax = plt.subplots(1, 2, figsize=(18,6))\n",
    "  ax[0].plot(dates, positive_ratios_100, c=\"g\", label=\"Positive Sentiment Ratio (%)\")\n",
    "  ax[0].plot(dates, SPY_prices, c=\"r\", label=\"SPY Price\")\n",
    "  ax[0].set_title(\"Positive Sentiment Ratios and SPY Prices Over 2 Trading Month Period\", weight=\"bold\")\n",
    "  ax[0].legend(loc=\"best\")\n",
    "  ax[1].plot(dates, positive_ratios_100, c=\"g\", label=\"Positive Sentiment Ratio (%)\")\n",
    "  ax[1].plot(dates, DJIA_prices, c=\"b\", label=\"DJIA Price\")\n",
    "  fig.autofmt_xdate(rotation=60)\n",
    "  fig.tight_layout()\n",
    "  ax[1].set_title(\"Positive Sentiment Ratios and DJIA Prices Over 2 Trading Month Period\", weight=\"bold\")\n",
    "  ax[1].legend(loc='best')\n",
    "  fig.show()\n",
    "  fig, ax = plt.subplots(1, 2, figsize=(18,6))\n",
    "  ax[0].plot(dates, positive_ratios, c=\"g\", label=\"Positive Sentiment Ratio\")\n",
    "  ax[0].plot(dates, SPY_returns, c=\"r\", label=\"SPY Returns (*100)\")\n",
    "  ax[0].set_title(\"Positive Sentiment Ratios and SPY Returns Over 2 Trading-Month Period\", weight=\"bold\")\n",
    "  ax[0].legend(loc=\"best\")\n",
    "  ax[1].plot(dates, positive_ratios, c=\"g\", label=\"Positive Sentiment Ratio\")\n",
    "  ax[1].plot(dates, DJIA_returns, c=\"b\", label=\"DJIA Returns (*100)\")\n",
    "  fig.autofmt_xdate(rotation=60)\n",
    "  fig.tight_layout()\n",
    "  ax[1].set_title(\"Positive Sentiment Ratios and DJIA Returns Over 2 Trading-Month Period\", weight=\"bold\")\n",
    "  ax[1].legend(loc='best')\n",
    "  fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E52QiZ0-6X1o"
   },
   "outputs": [],
   "source": [
    "pos_ratio_vs_day(signals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQWm5GAM7WV5"
   },
   "source": [
    "This graph shows that the great majority of tweets are postive. This does mean the BERT sentiment prediction model is broken, but instead implies that most stock-related tweets express positive sentiment. In essence, this makes sense since most people would only publicly broadcast their trading performance if they were doing well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVCgLLvXRqQ9"
   },
   "source": [
    "*3.9: Now, we can finally define our sentiment trading strategy. The strategy works by following the logic given by this pseudocode:*\n",
    "\n",
    "**For every day in our ETF price dataset, check the sentiment trading signal for this day (using the signals calculated earlier).** \n",
    "\n",
    "**If the sentiment signal > pos_threshold, go long k units of the ETF.**\n",
    "\n",
    "**Else if the sentiment signal < neg_threshold, sell k units of the ETF.** \n",
    "\n",
    "**Else, hold the current position.**\n",
    "\n",
    "**For every position taken, use the value of this position to update the portfolio value which starts at initial_portfolio_value.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DuhOUGvxTh9Q"
   },
   "outputs": [],
   "source": [
    "# ~\n",
    "# sentiment_trading_strategy() is the main function for this notebook. It takes \n",
    "# the following arguments:\n",
    "#         initial_portfolio_value\n",
    "#\n",
    "#\n",
    "#\n",
    "# ~If tweets for one day are majority positive go long 1 SPY and 1 DJIA, else sell k SPY and k DJIA\n",
    "def sentiment_trading_strategy(initial_portfolio_value, signals, k, ETF, pos_threshold, neg_threshold):\n",
    "  ETF = ETF + \" Prices\"\n",
    "  portfolio = []\n",
    "  prev_gain = initial_portfolio_value\n",
    "  portfolio.append(initial_portfolio_value) # We start by going long k units for the ETF\n",
    "  signals_indx = 0\n",
    "  indicators = []\n",
    "  for date in ETF_prices.index[5:-5].strftime('%Y-%m-%d'):\n",
    "    todays_price = ETF_prices.loc[date][ETF] # Get today's price \n",
    "    signal = signals[signals_indx]\n",
    "    if signal > pos_threshold:   # If the trading signal on the previous day is positive, then we go long k units of the ETF\n",
    "      indicators.append(1)\n",
    "      portfolio_value_change = k*todays_price \n",
    "      portfolio_value_gain = prev_gain + portfolio_value_change\n",
    "      portfolio.append(portfolio_value_gain) # Add the portfolio value gain to the portfolio\n",
    "    elif signal < neg_threshold:\n",
    "      indicators.append(-1)\n",
    "      portfolio_value_change = -(k*todays_price)\n",
    "      portfolio_value_gain = prev_gain + portfolio_value_change\n",
    "      portfolio.append(portfolio_value_gain)\n",
    "    else:\n",
    "      indicators.append(0)\n",
    "      portfolio.append(prev_gain)\n",
    "    prev_gain = portfolio_value_gain\n",
    "    signals_indx += 1\n",
    "  return portfolio, indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3M7fbw0ekylf"
   },
   "source": [
    "*3.10: We have now successfully implemented our sentiment-based ETF trading strategy! All that remains is to now optimize and analyze the performance of this strategy by finding which positive-negative threshold combination yields the most accurate indicators, and then we will use this combination for model performance analysis in Section 4. We will find the optimal positive-negative threshold combination using the get_optimal_pos_neg_thresholds() helper function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8TBYsqtvkcBx"
   },
   "outputs": [],
   "source": [
    "# ~\n",
    "#\n",
    "#\n",
    "# ~\n",
    "def signal_accuracy(indicators, returns):\n",
    "  correct_num = 0\n",
    "  total_num = 0\n",
    "  for i in range(len(indicators)):\n",
    "    if indicators[i] == 1:\n",
    "      total_num += 1\n",
    "      if returns[i] > 0:\n",
    "        correct_num += 1\n",
    "    elif indicators[i] == -1:\n",
    "      total_num += 1\n",
    "      if returns[i] < 0:\n",
    "        correct_num += 1\n",
    "  accuracy = correct_num / total_num \n",
    "  return accuracy \n",
    "\n",
    "  \n",
    "# ~\n",
    "# \n",
    "# ~\n",
    "pos_thresholds = [0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95]\n",
    "neg_thresholds = [0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95]\n",
    "def get_optimal_pos_neg_thresholds(pos_thresholds, neg_thresholds, returns):\n",
    "  best_pos_threshold = 0\n",
    "  best_neg_threshold = 0\n",
    "  best_accuracy = 0\n",
    "  for p in pos_thresholds:\n",
    "    for n in neg_thresholds:\n",
    "      _, indicators = sentiment_trading_strategy(5000, signals, 1, \"SPY\", p, n)\n",
    "      accuracy = signal_accuracy(indicators, returns)\n",
    "      if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_pos_threshold = p\n",
    "        best_neg_threshold = n\n",
    "  return p, n, best_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zq_fyQwn362"
   },
   "source": [
    "~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ \n",
    "\n",
    "**PART 4: Analyzing Performance of Optimal Sentiment-Based ETF Forecasting Strategy**\n",
    "\n",
    "~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oczzgr_jnN_G"
   },
   "source": [
    "With our sentiment-based ETF trading strategy defined and optimized, we can test the performance of this strategy by plotting portfolio results and by comparing these results against four other baseline trading strategies:\n",
    "\n",
    "\n",
    "1.   Risk-Free Rate\n",
    "2.   Buy-and-Hold\n",
    "3.   Predicting Returns with Ordinary Linear Regression \n",
    "4.   Predicting Returns with ElasticNet \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5gyPeCgMQv3F"
   },
   "outputs": [],
   "source": [
    "SPY_portfolio_1, SPY_signals = sentiment_trading_strategy(5000, 5, \"SPY\", 0.8, 0.77, 0.1, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HnwQkQIsgx9g"
   },
   "outputs": [],
   "source": [
    "DJIA_portfolio_1, DJIA_signals = sentiment_trading_strategy(5000, 5, \"DJIA\", 0.8, 0.1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUS8r2bOpMij"
   },
   "outputs": [],
   "source": [
    "print(SPY_portfolio_1)\n",
    "print(DJIA_portfolio_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AEF-akWS-ERj"
   },
   "outputs": [],
   "source": [
    "def plot_portfolio_results():\n",
    "  dates = [d for d in ETF_prices.index[4:-5].strftime('%Y-%m-%d')]\n",
    "  plt.plot(dates, SPY_portfolio_1, c='r', label='Tweet Sentiment Trading Strategy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 1088,
     "status": "ok",
     "timestamp": 1670801033455,
     "user": {
      "displayName": "Sasha Agapiev",
      "userId": "18182803548370990209"
     },
     "user_tz": 300
    },
    "id": "gQaK3JlLoGi2",
    "outputId": "964e8227-f21f-4b0b-ba18-b0f66a0ab8cf"
   },
   "outputs": [],
   "source": [
    "plot_portfolio_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQSk1ADXnb6M"
   },
   "outputs": [],
   "source": [
    "def sharpe_ratio(ts_returns, periods_per_year=252):\n",
    "    \"\"\"\n",
    "    sharpe_ratio - Calculates annualized return, annualized vol, and annualized sharpe ratio, \n",
    "                    where sharpe ratio is defined as annualized return divided by annualized volatility \n",
    "                    \n",
    "    Arguments:\n",
    "    ts_returns - pd.Series of returns of a single eigen portfolio\n",
    "    \n",
    "    Return:\n",
    "    a tuple of three doubles: annualized return, volatility, and sharpe ratio\n",
    "    \"\"\"\n",
    "    \n",
    "    annualized_return = 0.\n",
    "    annualized_vol = 0.\n",
    "    annualized_sharpe = 0.\n",
    "    \n",
    "    ### START CODE HERE ### ( 4-5 lines of code)\n",
    "    n_years = ts_returns.shape[0] / periods_per_year\n",
    "    annualized_return = np.power(np.prod(1 + ts_returns), (1 / n_years)) - 1\n",
    "    annualized_vol = ts_returns.std() * np.sqrt(periods_per_year)\n",
    "    annualized_sharpe = annualized_return / annualized_vol\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return annualized_return, annualized_vol, annualized_sharpe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TE7oUjdEl8vA"
   },
   "source": [
    "The two baseline strategies we will compare against are momentum strategy with OLS and PCA eigen portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_B2jlKUJl8Qf"
   },
   "outputs": [],
   "source": [
    "def get_annualized_returns(returns_list):\n",
    "  curr = 1\n",
    "  newlist = []\n",
    "  for ret in returns_list:\n",
    "    curr = curr * (1+ret)\n",
    "    newlist.append(curr)\n",
    "  return newlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "executionInfo": {
     "elapsed": 351,
     "status": "error",
     "timestamp": 1670802049404,
     "user": {
      "displayName": "Sasha Agapiev",
      "userId": "18182803548370990209"
     },
     "user_tz": 300
    },
    "id": "SJvXEhYKjnb9",
    "outputId": "a3321d73-0b80-4a81-d8bc-5e38b37cadcf"
   },
   "outputs": [],
   "source": [
    "spy_ret = get_annualized_returns(ETF_Returns.loc[\"SPY Returns\"])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyObEo7NNQJclcg3r9pGLIIx",
   "mount_file_id": "1KLhLT0zZhMzcFBmUhtXwBnxOIwqaz7k8",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
